conf:
  ceph:
    pools:
      backup:
        replication: 1
        crush_rule: same_host
        chunk_size: 8
        app_name: cinder-backup
      # default pool used by rbd1 backend
      cinder.volumes:
        replication: 1
        crush_rule: same_host
        chunk_size: 8
        app_name: cinder-volume
      # secondary pool used by rbd2 backend
      cinder.volumes.gold:
        replication: 1
        crush_rule: same_host
        chunk_size: 8
        app_name: cinder-volume
  backends:
    # add an extra storage backend same values as rbd1 (see
    # cinder/values.yaml) except for volume_backend_name and rbd_pool
    rbd2:
      volume_driver: cinder.volume.drivers.rbd.RBDDriver
      volume_backend_name: rbd2
      rbd_pool: cinder.volumes.gold
      rbd_ceph_conf: "/etc/ceph/ceph.conf"
      rbd_flatten_volume_from_snapshot: false
      report_discard_supported: true
      rbd_max_clone_depth: 5
      rbd_store_chunk_size: 4
      rados_connect_timeout: -1
      rbd_user: cinder
      rbd_secret_uuid: 457eb676-33da-42ec-9a8c-9293d545c337
pod:
  replicas:
    api: 1
    volume: 1
    scheduler: 1
    backup: 1
